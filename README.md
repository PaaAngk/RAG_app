Вот перевод инструкции на русский язык:

---

# Система RAG с Milvus, SentenceTransformers, BM25 и LLM

Это микросервисная система RAG (Retrieval-Augmented Generation), которая позволяет пользователю задавать вопросы и получать ответы на основе информации из Excel-файла. Система использует гибридный поиск (BM25 + векторный), чтобы находить релевантные данные, и генерирует ответ с помощью LLM.

## Возможности

- **Гибридный поиск**: сочетает плотный векторный поиск (SentenceTransformers) и разреженный поиск (BM25) для повышения качества извлечения
- **Настраиваемые веса**: возможность управлять балансом между схожестью эмбеддингов и совпадением по ключевым словам
- **Гибкость в выборе моделей эмбеддинга**: можно использовать разные модели SentenceTransformers
- **Внешняя векторная база**: подключение к внешнему экземпляру Milvus для масштабируемого поиска

## Архитектура

Система состоит из следующих компонентов:

1. **Search API**: сервис, объединяющий генерацию эмбеддингов и индексацию Excel-данных в Milvus  
2. **API**: шлюз между клиентом, search_api и LLM  
3. **Telegram-бот**: интерфейс для взаимодействия через Telegram  
4. **Milvus**: внешняя векторная база данных (опционально, можно запустить через docker-compose с профилем)

## Необходимые компоненты

- Docker и Docker Compose  
- Excel-файл с колонками "question" и "answer"  
- OpenAI API или совместимый endpoint LLM  
- (Опционально) токен Telegram-бота

## Установка

1. Клонируйте репозиторий  
2. Поместите ваш Excel-файл в директорию `data` (`qa_database.xlsx`) (или используйте конвертер тестовых данных)  
3. Настройте LLM в файле `docker-compose.yml`:  
   - Откройте `docker-compose.yml`  
   - В секции сервиса `api` настройте переменные окружения  
   - Один из вариантов:  
     - Вариант 1: OpenAI API — раскомментируйте и укажите ваш API-ключ  
     - Вариант 2: собственный endpoint (OpenAI API) LLM — раскомментируйте и укажите URL. Для локальной LLM использовать "http://host.docker.internal:1234/v1" (LMStudio)
   - Опционально: укажите переменную `LLM_MODEL`, чтобы задать модель  
   - Вы можете также изменить модель эмбеддинга через `EMBEDDING_MODEL` в `search_api`  
   - Настройте подключение к Milvus через `MILVUS_URI` (по умолчанию: http://milvus-standalone:19530)  
   - (Опционально) укажите `TELEGRAM_TOKEN`, чтобы активировать Telegram-бота

4. Запуск сервисов:
   ```bash
   # Без Milvus (если он уже запущен отдельно)
   docker-compose up -d

   # Или с локальным сервером Milvus
   docker-compose --profile milvus up -d
   ```

5. Остановка сервиса:
   ```bash
   docker compose --profile milvus down -v
   ```

## Использование

У каждого сервиса есть документация OpenAPI, доступная по адресу `/docs` для каждого сервиса.

**Search API: http://localhost:8000/docs**

**LLM API: http://localhost:8080/docs**

### Индексация данных

Перед выполнением запросов необходимо проиндексировать данные. Способы:

1. **По умолчанию используется Excel-файл**  
   Система ищет файл по пути, указанному в переменной окружения `EXCEL_PATH`.

2. **Через API индексатора**:  
   ```bash
   curl -X POST "http://localhost:8000/index/excel" \
        -H "Content-Type: application/x-www-form-urlencoded" \
        -d "file_path=/data/your_file.xlsx"
   ```

   Или отправить файл напрямую:  
   ```bash
   curl -X POST "http://localhost:8000/index/excel" \
        -F "file=@/path/to/local/file.xlsx"
   ```

3. **Проверка статуса коллекции**:  
   ```bash
   curl "http://localhost:8000/collection/status"
   ```

### Запросы

После индексации можно выполнять запросы к API:

```bash
# Простой запрос
curl -X POST "http://localhost:8080/query" \
     -H "Content-Type: application/json" \
     -d '{"question": "Что такое RAG?"}'
```

Запрос с настройкой гибридного поиска:
```bash
curl -X POST "http://localhost:8080/query" \
     -H "Content-Type: application/json" \
     -d '{
       "question": "Что такое RAG?",
       "top_k": 5,
       "use_hybrid": true,
       "vector_weight": 0.7,
       "keyword_weight": 0.3
     }'
```

Также можно обращаться напрямую к `search_api`:

```bash
curl -X POST "http://localhost:8000/search" \
     -H "Content-Type: application/json" \
     -d '{
       "query": "Что такое RAG?",
       "top_k": 5,
       "use_hybrid": true,
       "vector_weight": 0.7,
       "keyword_weight": 0.3
     }'
```

Ответ включает сгенерированный ответ и источники:

```json
{
  "answer": "RAG (Retrieval-Augmented Generation) — это подход, который усиливает возможности LLM, извлекая релевантную информацию из внешних источников знаний перед генерацией ответа...",
  "sources": [
    {
      "question": "Что такое RAG?",
      "answer": "RAG — это подход, усиливающий большие языковые модели...",
      "score": 0.98
    },
    ...
  ]
}
```

### Использование Telegram-бота

Если вы настроили Telegram-бота, можно работать с системой через Telegram:

1. **Получите токен бота**:  
   - Перейдите в [BotFather](https://t.me/botfather)  
   - Создайте бота командой `/newbot`  
   - Скопируйте токен

2. **Настройте переменную окружения**:  
   ```bash
   export TELEGRAM_TOKEN=ваш_токен
   ```  
   Или добавьте в `docker-compose.yml`

3. **Запустите сервисы**, как описано выше

4. **Общайтесь с ботом**:  
   - Найдите его по имени  
   - Начните с `/start`  
   - Задавайте вопросы в чате  
   - Используйте `/settings` для изменения параметров  
   - Команда `/help` покажет список доступных функций

## Настройка

- **Excel-данные**: замените пример своим Excel-файлом с QA-парами  
- **Модель эмбеддингов**: указывается в `EMBEDDING_MODEL`  
- **LLM**: используйте `OPENAI_API_KEY` или `LLM_URL` в `docker-compose.yml`  
- **Модель LLM**: указывается в переменной `LLM_MODEL`  
- **Кэш Hugging Face**: используется каталог `./volumes/huggingface_cache` для ускорения запуска и уменьшения трафика  
- **Milvus**: настраивается через `MILVUS_URI`  
- **Telegram-бот**: активируется переменной `TELEGRAM_TOKEN`

## Отладка

- **Проблемы с Milvus**: убедитесь, что Milvus запущен и доступен по адресу  
- **Аутентификация**: проверьте логин/пароль, если используются  
- **Ошибки генерации эмбеддингов**: проверьте, хватает ли памяти контейнеру  
- **Ошибки LLM**: проверьте API-ключ или адрес сервера  
- **Telegram-бот**: убедитесь, что токен верный и бот активен
