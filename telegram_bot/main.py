import os
import logging
import requests
import json
import datetime
from typing import Dict, Any, Optional
from telegram import Update, InlineKeyboardButton, InlineKeyboardMarkup
from telegram.ext import (
    Application,
    CommandHandler,
    MessageHandler,
    CallbackQueryHandler,
    ContextTypes,
    filters,
)
from openai import OpenAI

# Configure logging
logging.basicConfig(
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s", level=logging.INFO
)
logger = logging.getLogger(__name__)

# Configure file logging for requests and results
LOG_DIR = os.environ.get("LOG_DIR", "logs")
os.makedirs(LOG_DIR, exist_ok=True)

# Create a file handler for the request logger
request_logger = logging.getLogger("request_logger")
request_logger.setLevel(logging.INFO)

# Create log filename with current date
log_filename = os.path.join(LOG_DIR, f"telegram_requests_{datetime.datetime.now().strftime('%Y-%m-%d')}.log")
file_handler = logging.FileHandler(log_filename)
file_handler.setFormatter(logging.Formatter('%(asctime)s - %(message)s'))
request_logger.addHandler(file_handler)

# Environment variables
TELEGRAM_TOKEN = os.environ.get("TELEGRAM_TOKEN")
SEARCH_URL = os.environ.get("SEARCH_URL", "http://localhost:8000")
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")
LLM_URL = os.environ.get("LLM_URL", "http://localhost:1234/v1")
LLM_MODEL = os.environ.get("LLM_MODEL", "gpt-3.5-turbo")

# Global OpenAI client
llm_client = None

# Initialize OpenAI client
def initialize_llm_client():
    global llm_client

    if OPENAI_API_KEY:
        logger.info("Initializing OpenAI client with API key")
        llm_client = OpenAI(api_key=OPENAI_API_KEY)
    elif LLM_URL:
        logger.info(f"Initializing OpenAI client with custom base URL: {LLM_URL}")
        llm_client = OpenAI(base_url=LLM_URL, api_key="dummy_key")
    else:
        logger.warning("No LLM configuration provided (OPENAI_API_KEY or LLM_URL)")
        llm_client = None

    return llm_client is not None

# Search function
async def search_knowledge_base(query: str, top_k: int = 5, use_hybrid: bool = True) -> list:
    """Search the knowledge base using the indexer API"""
    try:
        search_params = {
            "query": query,
            "top_k": top_k,
            "use_hybrid": use_hybrid,
        }

        response = requests.post(
            f"{SEARCH_URL}/search",
            json=search_params,
            timeout=30
        )
        response.raise_for_status()

        return response.json()["results"]
    except Exception as e:
        logger.error(f"Error searching knowledge base: {str(e)}")
        return []

# Generate response using LLM
async def generate_llm_response(query: str, context_data: list) -> str:
    """Generate response using LLM"""
    try:
        if llm_client is None:
            return "Sorry, the LLM service is not available at the moment."

        # Prepare context from retrieved QA pairs
        context = ""
        for i, item in enumerate(context_data):
            context += f"Context {i+1}:\nQuestion: {item['question']}\nAnswer: {item['answer']}\n\n"

        # Create prompt
        prompt = f"""–¢—ã —ç–∫—Å–ø–µ—Ä—Ç-–∫—Ä–∏–º–∏–Ω–∞–ª–∏—Å—Ç. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ø—Ä–∏–≤–µ–¥—ë–Ω–Ω—ã–µ –Ω–∏–∂–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã (Context 1, Context 2 –∏ —Ç.¬†–¥.) –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.
–ï—Å–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–∞–Ω–∏–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ —Å—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç, —Å–∫–∞–∂–∏—Ç–µ ¬´–£ –º–µ–Ω—è –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, —á—Ç–æ–±—ã –æ—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ —ç—Ç–æ—Ç –≤–æ–ø—Ä–æ—Å¬ª.

{context}

User Question: {query}

Answer:"""

        # Log which LLM model we're using
        logger.info(f"Using LLM model: {LLM_MODEL}")

        # Make the API call using the global client
        response = llm_client.chat.completions.create(
            model=LLM_MODEL,
            messages=[
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3,
            max_tokens=1000
        )

        return response.choices[0].message.content

    except Exception as e:
        logger.error(f"Error generating LLM response: {str(e)}")
        return f"–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–∏ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–∏ –æ—Ç–≤–µ—Ç–∞ –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {str(e)}"

# Command handlers
async def start(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """Send a message when the command /start is issued."""
    user = update.effective_user
    await update.message.reply_html(
        f"–ü—Ä–∏–≤–µ—Ç  {user.mention_html()}! –Ø —Ç–≤–æ–π –ø–æ–º–æ—â–Ω–∏–∫ RAG. –ó–∞–¥–∞–π –º–Ω–µ –ª—é–±–æ–π –≤–æ–ø—Ä–æ—Å, –∏ —è –ø–æ—Å—Ç–∞—Ä–∞—é—Å—å –Ω–∞–π—Ç–∏ –æ—Ç–≤–µ—Ç –≤ —Å–≤–æ–µ–π –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π. –í–≤–µ–¥–∏ /help –¥–ª—è –ø–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–µ–π."
    )

async def help_command(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """Send a message when the command /help is issued."""
    help_text = """
*–ü–æ–º–æ—â—å —Å –±–æ—Ç–æ–º RAG –ø–æ –∫—Ä–∏–º–∏–Ω–∞–ª–∏—Å—Ç–∏–∫–µ*

–≠—Ç–æ—Ç –±–æ—Ç –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—é —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö (RAG), —á—Ç–æ–±—ã –æ—Ç–≤–µ—á–∞—Ç—å –Ω–∞ –≤–∞—à–∏ –≤–æ–ø—Ä–æ—Å—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π.

*Commands:*
/start - Start the bot
/help - –ü–æ–∫–∞–∑–∞—Ç—å —ç—Ç–æ —Å–ø—Ä–∞–≤–æ—á–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
/settings - –ù–∞—Å—Ç—Ä–æ–π—Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ–∏—Å–∫–∞

*–ö–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å:*
–ü—Ä–æ—Å—Ç–æ –≤–≤–µ–¥–∏—Ç–µ —Å–≤–æ–π –≤–æ–ø—Ä–æ—Å, –∏ —è –Ω–∞–π–¥—É —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä—É—é –æ—Ç–≤–µ—Ç.

*–ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ–∏—Å–∫–∞:*
–¢—ã –º–æ–∂–µ—à—å –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å —Å–ø–æ—Å–æ–± –ø–æ–∏—Å–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ —Å –ø–æ–º–æ—â—å—é –∫–æ–º–∞–Ω–¥—ã /settings.
- –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫: –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ –∏ –ø–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
    """
    await update.message.reply_markdown(help_text)

async def settings_command(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """Handle the /settings command."""
    keyboard = [
        [
            InlineKeyboardButton("–ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫: ON", callback_data="hybrid_on"),
            InlineKeyboardButton("–ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫: OFF", callback_data="hybrid_off"),
        ],
    ]
    reply_markup = InlineKeyboardMarkup(keyboard)
    await update.message.reply_text("–ù–∞—Å—Ç—Ä–æ–∏—Ç—å –ø–æ–∏—Å–∫:", reply_markup=reply_markup)

async def button_callback(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """Handle button callbacks."""
    query = update.callback_query
    await query.answer()

    # Get user data or initialize if not exists
    if not context.user_data.get("search_settings"):
        context.user_data["search_settings"] = {
            "use_hybrid": True,
        }

    # Handle different callbacks
    if query.data == "hybrid_on":
        context.user_data["search_settings"]["use_hybrid"] = True
        await query.edit_message_text("–ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω ON")

    elif query.data == "hybrid_off":
        context.user_data["search_settings"]["use_hybrid"] = False
        await query.edit_message_text("–ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω OFF")

    elif query.data == "settings_done":
        settings = context.user_data["search_settings"]
        settings_text = f"""
*–¢–µ–∫—É—â–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏:*
- –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫: {'ON' if settings['use_hybrid'] else 'OFF'}
        """
        await query.edit_message_text(settings_text, parse_mode="Markdown")

async def handle_message(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """Handle user messages."""
    # Get user's question
    question = update.message.text
    user_id = update.effective_user.id
    username = update.effective_user.username or "unknown"

    # Log the user request
    request_logger.info(f"USER_REQUEST | User: {username} (ID: {user_id}) | Question: {question}")

    # Get search settings from user data or use defaults
    search_settings = context.user_data.get("search_settings", {
        "use_hybrid": True,
    })

    # Send typing action
    await update.message.chat.send_action("typing")

    # First, let the user know we're processing their question
    processing_message = await update.message.reply_text("üîç –ò—â—É –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é...")

    try:
        # Search for relevant information
        search_results = await search_knowledge_base(
            query=question,
            top_k=3,
            use_hybrid=search_settings["use_hybrid"],
        )

        # Log search results
        search_results_log = json.dumps(search_results, ensure_ascii=False) if search_results else "No results found"
        request_logger.info(f"SEARCH_RESULTS | User: {username} (ID: {user_id}) | Query: {question} | Results: {search_results_log}")

        if not search_results:
            await processing_message.edit_text("–ú–Ω–µ –Ω–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –Ω–∏–∫–∞–∫–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, –∫–æ—Ç–æ—Ä–∞—è –º–æ–≥–ª–∞ –±—ã –æ—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ –≤–∞—à –≤–æ–ø—Ä–æ—Å.")
            return

        # Update message to show we're generating a response
        await processing_message.edit_text("ü§î –°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞...")

        # Generate response using LLM
        answer = await generate_llm_response(question, search_results)

        # Log the generated response
        request_logger.info(f"GENERATED_RESPONSE | User: {username} (ID: {user_id}) | Question: {question} | Response: {answer[:500]}{'...' if len(answer) > 500 else ''}")

        # Prepare sources text
        sources_text = "\n\n*–ò—Å—Ç–æ—á–Ω–∏–∫–∏:*\n"
        for i, source in enumerate(search_results):
            # Truncate long answers
            short_answer = source["answer"][:100] + "..." if len(source["answer"]) > 100 else source["answer"]
            sources_text += f"{i+1}. Q: {source['question']}\nA: {short_answer}\n"

        # Send the final answer with sources
        response_text = f"{answer}{sources_text}"

        # If response is too long, split it
        if len(response_text) > 4000:
            await processing_message.edit_text(answer[:4000])
            await update.message.reply_text(sources_text[:4000], parse_mode="Markdown")
        else:
            await processing_message.edit_text(response_text, parse_mode="Markdown")

    except Exception as e:
        error_message = f"Error processing message: {str(e)}"
        logger.error(error_message)
        request_logger.error(f"ERROR | User: {username} (ID: {user_id}) | Question: {question} | Error: {error_message}")
        await processing_message.edit_text(f"–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {str(e)}")

def main() -> None:
    """Start the bot."""
    # Check if token is provided
    if not TELEGRAM_TOKEN:
        logger.error("TELEGRAM_TOKEN environment variable is not set!")
        return

    # Initialize LLM client
    if not initialize_llm_client():
        logger.warning("Failed to initialize LLM client. Bot will start but may not generate responses.")

    # Create the Application
    application = Application.builder().token(TELEGRAM_TOKEN).build()

    # Add handlers
    application.add_handler(CommandHandler("start", start))
    application.add_handler(CommandHandler("help", help_command))
    application.add_handler(CommandHandler("settings", settings_command))
    application.add_handler(CallbackQueryHandler(button_callback))

    # Handle messages
    application.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_message))

    # Run the bot
    application.run_polling(allowed_updates=Update.ALL_TYPES)

if __name__ == "__main__":
    main()
