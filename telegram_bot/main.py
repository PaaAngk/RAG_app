import os
import logging
import requests
import json
import datetime
from typing import Dict, Any, Optional
from telegram import Update, InlineKeyboardButton, InlineKeyboardMarkup
from telegram.ext import (
    Application,
    CommandHandler,
    MessageHandler,
    CallbackQueryHandler,
    ContextTypes,
    filters,
)
from openai import OpenAI

# Configure logging
logging.basicConfig(
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s", level=logging.INFO
)
logger = logging.getLogger(__name__)

# Configure file logging for requests and results
LOG_DIR = os.environ.get("LOG_DIR", "logs")
os.makedirs(LOG_DIR, exist_ok=True)

# Create a file handler for the request logger
request_logger = logging.getLogger("request_logger")
request_logger.setLevel(logging.INFO)

# Create log filename with current date
log_filename = os.path.join(LOG_DIR, f"telegram_requests_{datetime.datetime.now().strftime('%Y-%m-%d')}.log")
file_handler = logging.FileHandler(log_filename)
file_handler.setFormatter(logging.Formatter('%(asctime)s - %(message)s'))
request_logger.addHandler(file_handler)

# Environment variables
TELEGRAM_TOKEN = os.environ.get("TELEGRAM_TOKEN")
SEARCH_URL = os.environ.get("SEARCH_URL", "http://localhost:8000")
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")
LLM_URL = os.environ.get("LLM_URL", "http://localhost:1234/v1")
LLM_MODEL = os.environ.get("LLM_MODEL", "gpt-3.5-turbo")

# Global OpenAI client
llm_client = None

# Initialize OpenAI client
def initialize_llm_client():
    global llm_client

    if OPENAI_API_KEY:
        logger.info("Initializing OpenAI client with API key")
        llm_client = OpenAI(api_key=OPENAI_API_KEY)
    elif LLM_URL:
        logger.info(f"Initializing OpenAI client with custom base URL: {LLM_URL}")
        llm_client = OpenAI(base_url=LLM_URL, api_key="dummy_key")
    else:
        logger.warning("No LLM configuration provided (OPENAI_API_KEY or LLM_URL)")
        llm_client = None

    return llm_client is not None

# Search function
async def search_knowledge_base(query: str, top_k: int = 5, use_hybrid: bool = True) -> list:
    """Search the knowledge base using the indexer API"""
    try:
        search_params = {
            "query": query,
            "top_k": top_k,
            "use_hybrid": use_hybrid,
        }

        response = requests.post(
            f"{SEARCH_URL}/search",
            json=search_params,
            timeout=30
        )
        response.raise_for_status()

        return response.json()["results"]
    except Exception as e:
        logger.error(f"Error searching knowledge base: {str(e)}")
        return []

# Generate response using LLM
async def generate_llm_response(query: str, context_data: list) -> str:
    """Generate response using LLM"""
    try:
        if llm_client is None:
            return "Sorry, the LLM service is not available at the moment."

        # Prepare context from retrieved QA pairs
        context = ""
        for i, item in enumerate(context_data):
            context += f"Context {i+1}:\nQuestion: {item['question']}\nAnswer: {item['answer']}\n\n"

        # Create prompt
        prompt = f"""Ð¢Ñ‹ ÑÐºÑÐ¿ÐµÑ€Ñ‚-ÐºÑ€Ð¸Ð¼Ð¸Ð½Ð°Ð»Ð¸ÑÑ‚. Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹Ñ‚Ðµ Ð¿Ñ€Ð¸Ð²ÐµÐ´Ñ‘Ð½Ð½Ñ‹Ðµ Ð½Ð¸Ð¶Ðµ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð½Ñ‹Ðµ Ð¼Ð°Ñ‚ÐµÑ€Ð¸Ð°Ð»Ñ‹ (Context 1, Context 2 Ð¸ Ñ‚.Â Ð´.) Ð´Ð»Ñ Ð¾Ñ‚Ð²ÐµÑ‚Ð° Ð½Ð° Ð²Ð¾Ð¿Ñ€Ð¾Ñ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ.
Ð•ÑÐ»Ð¸ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ð°Ð½Ð¸Ð¸ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð° Ð½ÐµÐ²Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ð¾ ÑÑ„Ð¾Ñ€Ð¼ÑƒÐ»Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð¾Ñ‚Ð²ÐµÑ‚, ÑÐºÐ°Ð¶Ð¸Ñ‚Ðµ Â«Ð£ Ð¼ÐµÐ½Ñ Ð½ÐµÐ´Ð¾ÑÑ‚Ð°Ñ‚Ð¾Ñ‡Ð½Ð¾ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ð¸, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð¾Ñ‚Ð²ÐµÑ‚Ð¸Ñ‚ÑŒ Ð½Ð° ÑÑ‚Ð¾Ñ‚ Ð²Ð¾Ð¿Ñ€Ð¾ÑÂ».

{context}

User Question: {query}

Answer:"""

        # Log which LLM model we're using
        logger.info(f"Using LLM model: {LLM_MODEL}")

        # Make the API call using the global client
        response = llm_client.chat.completions.create(
            model=LLM_MODEL,
            messages=[
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3,
            max_tokens=1000
        )

        return response.choices[0].message.content

    except Exception as e:
        logger.error(f"Error generating LLM response: {str(e)}")
        return f"Ð˜Ð·Ð²Ð¸Ð½Ð¸Ñ‚Ðµ, Ð¿Ñ€Ð¸ Ñ„Ð¾Ñ€Ð¼Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ð¸ Ð¾Ñ‚Ð²ÐµÑ‚Ð° Ð¿Ñ€Ð¾Ð¸Ð·Ð¾ÑˆÐ»Ð° Ð¾ÑˆÐ¸Ð±ÐºÐ°: {str(e)}"

# Command handlers
async def start(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """Send a message when the command /start is issued."""
    user = update.effective_user
    await update.message.reply_html(
        f"ÐŸÑ€Ð¸Ð²ÐµÑ‚  {user.mention_html()}! Ð¯ Ñ‚Ð²Ð¾Ð¹ Ð¿Ð¾Ð¼Ð¾Ñ‰Ð½Ð¸Ðº RAG. Ð—Ð°Ð´Ð°Ð¹ Ð¼Ð½Ðµ Ð»ÑŽÐ±Ð¾Ð¹ Ð²Ð¾Ð¿Ñ€Ð¾Ñ, Ð¸ Ñ Ð¿Ð¾ÑÑ‚Ð°Ñ€Ð°ÑŽÑÑŒ Ð½Ð°Ð¹Ñ‚Ð¸ Ð¾Ñ‚Ð²ÐµÑ‚ Ð² ÑÐ²Ð¾ÐµÐ¹ Ð±Ð°Ð·Ðµ Ð·Ð½Ð°Ð½Ð¸Ð¹. Ð’Ð²ÐµÐ´Ð¸ /help Ð´Ð»Ñ Ð¿Ð¾Ð´Ñ€Ð¾Ð±Ð½Ð¾ÑÑ‚ÐµÐ¹."
    )

async def help_command(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """Send a message when the command /help is issued."""
    help_text = """
*ÐŸÐ¾Ð¼Ð¾Ñ‰ÑŒ Ñ Ð±Ð¾Ñ‚Ð¾Ð¼ RAG Ð¿Ð¾ ÐºÑ€Ð¸Ð¼Ð¸Ð½Ð°Ð»Ð¸ÑÑ‚Ð¸ÐºÐµ*

Ð­Ñ‚Ð¾Ñ‚ Ð±Ð¾Ñ‚ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ Ñ‚ÐµÑ…Ð½Ð¾Ð»Ð¾Ð³Ð¸ÑŽ Ñ€Ð°ÑÑˆÐ¸Ñ€ÐµÐ½Ð½Ð¾Ð¹ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð´Ð°Ð½Ð½Ñ‹Ñ… (RAG), Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð¾Ñ‚Ð²ÐµÑ‡Ð°Ñ‚ÑŒ Ð½Ð° Ð²Ð°ÑˆÐ¸ Ð²Ð¾Ð¿Ñ€Ð¾ÑÑ‹ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ Ð±Ð°Ð·Ñ‹ Ð·Ð½Ð°Ð½Ð¸Ð¹.

*Commands:*
/start - Start the bot
/help - ÐŸÐ¾ÐºÐ°Ð·Ð°Ñ‚ÑŒ ÑÑ‚Ð¾ ÑÐ¿Ñ€Ð°Ð²Ð¾Ñ‡Ð½Ð¾Ðµ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ
/settings - ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹Ñ‚Ðµ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹ Ð¿Ð¾Ð¸ÑÐºÐ°

*ÐšÐ°Ðº Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒ:*
ÐŸÑ€Ð¾ÑÑ‚Ð¾ Ð²Ð²ÐµÐ´Ð¸Ñ‚Ðµ ÑÐ²Ð¾Ð¹ Ð²Ð¾Ð¿Ñ€Ð¾Ñ, Ð¸ Ñ Ð½Ð°Ð¹Ð´Ñƒ ÑÐ¾Ð¾Ñ‚Ð²ÐµÑ‚ÑÑ‚Ð²ÑƒÑŽÑ‰ÑƒÑŽ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÑŽ Ð¸ ÑÐ³ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÑŽ Ð¾Ñ‚Ð²ÐµÑ‚.

*ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ¸ Ð¿Ð¾Ð¸ÑÐºÐ°:*
Ð¢Ñ‹ Ð¼Ð¾Ð¶ÐµÑˆÑŒ Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¸Ñ‚ÑŒ ÑÐ¿Ð¾ÑÐ¾Ð± Ð¿Ð¾Ð¸ÑÐºÐ° Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ð¸ Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ ÐºÐ¾Ð¼Ð°Ð½Ð´Ñ‹ /settings.
- Ð“Ð¸Ð±Ñ€Ð¸Ð´Ð½Ñ‹Ð¹ Ð¿Ð¾Ð¸ÑÐº: Ð¾Ð±ÑŠÐµÐ´Ð¸Ð½ÑÐµÑ‚ Ð²ÐµÐºÑ‚Ð¾Ñ€Ð½Ñ‹Ð¹ Ð¿Ð¾Ð¸ÑÐº Ð¸ Ð¿Ð¾Ð¸ÑÐº Ð¿Ð¾ ÐºÐ»ÑŽÑ‡ÐµÐ²Ñ‹Ð¼ ÑÐ»Ð¾Ð²Ð°Ð¼
    """
    await update.message.reply_markdown(help_text)

async def settings_command(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """Handle the /settings command."""
    keyboard = [
        [
            InlineKeyboardButton("Ð“Ð¸Ð±Ñ€Ð¸Ð´Ð½Ñ‹Ð¹ Ð¿Ð¾Ð¸ÑÐº: ON", callback_data="hybrid_on"),
            InlineKeyboardButton("Ð“Ð¸Ð±Ñ€Ð¸Ð´Ð½Ñ‹Ð¹ Ð¿Ð¾Ð¸ÑÐº: OFF", callback_data="hybrid_off"),
        ],
    ]
    reply_markup = InlineKeyboardMarkup(keyboard)
    await update.message.reply_text("ÐÐ°ÑÑ‚Ñ€Ð¾Ð¸Ñ‚ÑŒ Ð¿Ð¾Ð¸ÑÐº:", reply_markup=reply_markup)

async def button_callback(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """Handle button callbacks."""
    query = update.callback_query
    await query.answer()

    # Get user data or initialize if not exists
    if not context.user_data.get("search_settings"):
        context.user_data["search_settings"] = {
            "use_hybrid": True,
        }

    # Handle different callbacks
    if query.data == "hybrid_on":
        context.user_data["search_settings"]["use_hybrid"] = True
        await query.edit_message_text("Ð“Ð¸Ð±Ñ€Ð¸Ð´Ð½Ñ‹Ð¹ Ð¿Ð¾Ð¸ÑÐº Ð¿ÐµÑ€ÐµÐºÐ»ÑŽÑ‡ÐµÐ½ ON")

    elif query.data == "hybrid_off":
        context.user_data["search_settings"]["use_hybrid"] = False
        await query.edit_message_text("Ð“Ð¸Ð±Ñ€Ð¸Ð´Ð½Ñ‹Ð¹ Ð¿Ð¾Ð¸ÑÐº Ð¿ÐµÑ€ÐµÐºÐ»ÑŽÑ‡ÐµÐ½ OFF")

    elif query.data == "settings_done":
        settings = context.user_data["search_settings"]
        settings_text = f"""
*Ð¢ÐµÐºÑƒÑ‰Ð¸Ðµ Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ¸:*
- Ð“Ð¸Ð±Ñ€Ð¸Ð´Ð½Ñ‹Ð¹ Ð¿Ð¾Ð¸ÑÐº: {'ON' if settings['use_hybrid'] else 'OFF'}
        """
        await query.edit_message_text(settings_text, parse_mode="Markdown")

async def handle_message(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """Handle user messages."""
    # Get user's question
    question = update.message.text
    user_id = update.effective_user.id
    username = update.effective_user.username or "unknown"

    # Log the user request
    request_logger.info(f"USER_REQUEST | User: {username} (ID: {user_id}) | Question: {question}")

    # Get search settings from user data or use defaults
    search_settings = context.user_data.get("search_settings", {
        "use_hybrid": True,
    })

    # Send typing action
    await update.message.chat.send_action("typing")

    # First, let the user know we're processing their question
    processing_message = await update.message.reply_text("ðŸ” Ð˜Ñ‰Ñƒ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÑŽ...")

    try:
        # Search for relevant information
        search_results = await search_knowledge_base(
            query=question,
            top_k=3,
            use_hybrid=search_settings["use_hybrid"],
        )

        # Log search results
        search_results_log = json.dumps(search_results, ensure_ascii=False) if search_results else "No results found"
        request_logger.info(f"SEARCH_RESULTS | User: {username} (ID: {user_id}) | Query: {question} | Results: {search_results_log}")

        if not search_results:
            await processing_message.edit_text("ÐœÐ½Ðµ Ð½Ðµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð½Ð°Ð¹Ñ‚Ð¸ Ð½Ð¸ÐºÐ°ÐºÐ¾Ð¹ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ð¸, ÐºÐ¾Ñ‚Ð¾Ñ€Ð°Ñ Ð¼Ð¾Ð³Ð»Ð° Ð±Ñ‹ Ð¾Ñ‚Ð²ÐµÑ‚Ð¸Ñ‚ÑŒ Ð½Ð° Ð²Ð°Ñˆ Ð²Ð¾Ð¿Ñ€Ð¾Ñ.")
            return

        # Update message to show we're generating a response
        await processing_message.edit_text("ðŸ¤” Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð¾Ñ‚Ð²ÐµÑ‚Ð°...")

        # Generate response using LLM
        answer = await generate_llm_response(question, search_results)

        # Log the generated response
        request_logger.info(f"GENERATED_RESPONSE | User: {username} (ID: {user_id}) | Question: {question} | Response: {answer[:500]}{'...' if len(answer) > 500 else ''}")

        # Prepare sources text
        sources_text = "\n\n*Ð˜ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ¸:*\n"
        for i, source in enumerate(search_results):
            # Truncate long answers
            short_answer = source["answer"][:100] + "..." if len(source["answer"]) > 100 else source["answer"]
            sources_text += f"{i+1}. Q: {source['question']}\nA: {short_answer}\n"

        # Send the final answer with sources
        response_text = f"{answer}{sources_text}"

        # If response is too long, split it
        if len(response_text) > 4000:
            await processing_message.edit_text(answer[:4000])
            await update.message.reply_text(sources_text[:4000], parse_mode="Markdown")
        else:
            await processing_message.edit_text(response_text, parse_mode="Markdown")

    except Exception as e:
        error_message = f"Error processing message: {str(e)}"
        logger.error(error_message)
        request_logger.error(f"ERROR | User: {username} (ID: {user_id}) | Question: {question} | Error: {error_message}")
        await processing_message.edit_text(f"Ð˜Ð·Ð²Ð¸Ð½Ð¸Ñ‚Ðµ, Ð¿Ñ€Ð¾Ð¸Ð·Ð¾ÑˆÐ»Ð° Ð¾ÑˆÐ¸Ð±ÐºÐ°: {str(e)}")

def main() -> None:
    """Start the bot."""
    # Check if token is provided
    if not TELEGRAM_TOKEN:
        logger.error("TELEGRAM_TOKEN environment variable is not set!")
        return

    # Initialize LLM client
    if not initialize_llm_client():
        logger.warning("Failed to initialize LLM client. Bot will start but may not generate responses.")

    # Create the Application
    application = Application.builder().token(TELEGRAM_TOKEN).build()

    # Add handlers
    application.add_handler(CommandHandler("start", start))
    application.add_handler(CommandHandler("help", help_command))
    application.add_handler(CommandHandler("settings", settings_command))
    application.add_handler(CallbackQueryHandler(button_callback))

    # Handle messages
    application.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_message))

    # Run the bot
    application.run_polling(allowed_updates=Update.ALL_TYPES)

if __name__ == "__main__":
    main()
